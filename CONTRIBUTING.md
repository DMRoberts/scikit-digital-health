# Coding Standards
Everything inside the `/src` directory should follow PEP8 standards, and pass tests run by `flake8`, which is checked as part of the CI process on any pull requests into the `master` branch. There are 2 exceptions which are not checked (specified in `.flake8`): 
1. in `__init__.py` files, * imports may be used, and imports may be unused
2. Line length is set to a maximum of 99 (Python standard is 79, which is a bit short)

In order to make sure these standards are met, __before__ issuing a pull request, please check that your code conforms to these standards through the following:

1. Make sure flake8 is installed - `conda install flake8` or `pip install flake8` (note that the python version here is important - flake8 will test against syntax for the version it is installed alongside)
2. Navigate to the root directory and run `flake8 src/`
3. Make sure there are no warnings. If there are no warnings, the code is set to be in a pull request

# Automatic Testing
Through GitHub Actions, the test suite (using `pytest`) will be run through on different OSs and python versions. To make sure that your tests are working properly (at least on your OS/python version), at the very least you should run `pytest` from the root directory before any pull requests

# Adding modules
The goal of `scikit-imu` is to have one package with a defined architechture that allows for easy pipeline generation with multiple stages that may or may not depend on previous stages. To that end, there are several pre-defined base classes that will help setting up modules that are intended to directly interface with the pipeline infrastructure.

While you should fully read this document, there is the option to create a template module that will take care of some of the boilerplate that has to be present (and is described below). If you want to use this, from the root directory, run

```shell script
python templates/autocreate_module.py
```

You will be prompted for the module name, author name, and a short description, after which the module will be created in `src/skimu`

### 1. Create a new module directory
Under `src/skimu/` create a new directory with the desired name (for this example, we will use `preprocessing`), and create the normal files for a new python package (`__init__.py`, etc)

### 2. Create the module class that will be added to the pipeline
Below is an example file that contains the class that will be added to the pipeline to use its processing

```python
# src/skimu/preprocessing/preprocessing.py
import ...  # import installed modules (eg numpy, etc)

from skimu.base import _BaseProcess  # import the base process class

class PreProcessing(_BaseProcess):
    """
    Class to implement any preprocessing steps

    NOTE that this is the docstring for the class! Placing up here aids with clean autogenerated documentation

    ...
    """

    """
    Defining __repr__ is SUPER important!
    it is used for logging, and making sure that each process has a clear representation for pipelines, etc

    Don't split the lines here (ie with \\n), just make it one (potentially very) long line
    """
    def __repr__(self): 
        ret = "PreProcessing("  # class call/name + (
        # first __init__ parameter and its value. Note the ...!r} part, this calls the arguments own __repr__
        ret += f"attr1={self.attr1!r}, "
        ret += f"attr2={self.attr2!r})"  # second __init__ parameter and its value. note the ending ")"
        return ret
    """
    End result of PreProcessing.__repr__:
    PreProcessing(attr1=None, attr2=None)
    """

    def __init__(self, attr1=None, attr2=None):
        # make sure to call the super method
        # double check the docstring of _BaseProcess.__init__ for the required parameters 
        super().__init__('Preprocessing', False)
        # param 1 is a human-readable name for the process
        # param 2 is whether or not the result return value is important. 
        # If False, the input/output dictionary is returned from predict
        # If True, the result dictionary is returned

        self.attr1 = attr1
        ...
    
    """
    Due to the way that python sets docstrings, this call is necessary to have a publically viewable docstring
    for the public method, predict
    The definition and body of the function should not change 
    (ie always should be "def predict(self, *args, **kwargs)" and "super().predict(*args, **kwargs)"). 
    The only thing that gets modified here is the docstring, which should match that of the _predict declaration
    """
    def predict(self, *args, **kwargs):
        """
        Do the preprocessing step

        Paramters
        ---------
        time : numpy.ndarray
            (N, ) array of timestamps in unix time, in seconds (seconds since 1970/0/0 0:00:00)
        accel : numpy.ndarray
            (N, 3) array of acceleration values, with units of 'g'
        """
        return super().predict(*args, **kwargs)
    
    """
    This is the function that will actually run in the pipeline. It needs to have the call - 
    
    _predict(self, arg1=None, arg2=None, *, arg3=None, **kwargs). 
    
    This call would indicate that arg1 and arg2 are required (even though they have default options), 
    and arg3 is optional (ie might add some additional functionality). 
    
    Finally, the lone '*' argument indicates that everything after it must be passed as a key-word argument. 
    If you have no optional arguments (no arg3 in this case), the 
    call becomes: 
    
    _predict(self, arg1=None, arg2=None, **kwargs). 

    NOTE the units for time are [seconds since 1970/0/0 00:00:00], acceleration [g], angular velocity [deg/s]
    """
    def _predict(self, time=None, accel=None, **kwargs):

        # do any preprocessing - this can either be functions that are referenced here, or just all inside _predict
        accel1 = step1(accel)
        accel2 = step2(accel)

        # If you need something passed in thats not a default/standard argument 
        # (ie it might come through kwargs), use the following:
        if necessary_item in kwargs:
            nec_item = kwargs[necessary_item]
        else:
            raise KeyError(f'{necessary_item} not in the additional arguments passed into predict')

        """
        FINALLY: the return of the _predict function needs to be 2 items
        1. a dictionary of the input to _predict, and anything that might be needed in other stages
        2. anything else that needs to be returned, results, etc

        The dictionary from 1. MUST be updated with any arguments 
        that are passed in as required or optional with keywords!
        
        For this specific case, the goal is to modify the acceleration 
        (hence preprocessing, the modified version needs to be returned in the input dictionary)
        additionally, there are no specific results from this step, so None is returned as the second argument
        
        The neatest way to return the inputs, plus anything declared in the function declaration 
        is to update the kwargs variable, and return it
        """
        kwargs.update({self._time: time, self._acc: accel2})
        """
        Note that the _BaseProcess class has several attributes which help keep 
        track of the names of time, accel, etc, which should help minimize
        work if these names ever change. However, they can't be readily used in function declarations.
        """
        return kwargs, None
```

#### 2a. External file functions
If there is too much code to be contained inside the `PreProcessing._predict` call, there are a few suggested guidelines:

1. Generally, avoid adding too many other functions to the main file
2. Individual functions (especially if they are fairly long) should get their own file, with the name matching that of the function inside
3. Functions with a common theme can live in 1 file, with the common name matching that of the file
4. A "utility.py" file might make sense for any functions that have general utility *outside* of this specific module (ie something from `preprocessing/utility.py` getting called from `gait/gait.py`)

For examples of these, check out the `gait` module within `scikit-imu` - there are examples of 1 function per file, as well as multiple functions per file (`get_gait_metrics.py`)

However, these are just guidelines in order to maintain some clarity with multiple different functions split over multiple files. If you have a good reason to do something different, just try to maintain clarity for future users.

#### 2b. Structure for helper functions
There is no defined/suggested structure for the above helper functions, if a class makes sense for this instead of a function, then use a class, but you __definitely should__ add a `__repr__` method for that class, in case it has to be accessed from somewhere else.

An example of this can be seen again in the `sit2stand` module, where `detector.py` contains a `Detector` class that is used in `sit2stand.py`, both in the code as a helper function, but *also* inside the `Sit2Stand.__repr__`

### 3. Make sure everything is setup/imported
Make sure all imports are handled in `src/skimu/preprocessing/__init__.py`, as well as adding `preprocessing` imports to the `src/skimu/__init__.py`.

### 4. Make any additions to setup.py
If you don't have any data files (any non python files that need to be distributed with the package), or low level (c, cython, or fortran) extensions, everything should be good for the actual module, and you can skip to the Testing section

If you do have data files or extensions, do the following in `setup.py` (the main one in the root directory)

#### 4a. Data Files
For data files, find the `def configuration` function in `setup.py`, and locate the DATA FILES section, and add any data files that you have. If you have a lot of files in one directory, you can add a whole directory (but be careful, random files such as caches will be included as well).data

This requires a specific format, namely a tuple of (*desired_path*, *source_path*):

```python
# setup.py
...

def configuration(parent_package='', top_path=None):
    ...
    # DATA FILES
    # ========================
    config.add_data_files(
        ('skimu/gait/model', 'src/skimu/gait/model/final_features.json'),
        ('skimu/gait/model', 'src/skimu/gait/model/lgbm_gait_classifier_no-stairs.lgbm'),
        ('skimu/preprocessing/data', 'src/skimu/preprocessing/data/preprocessing_info.dat')        # Added this file
    )

    # alternatively add this directory, any files/folders under this directory will be added recursively
    config.add_data_dir('src/skimu/preprocessing/data')
    # ========================

    config.get_version('src/skimu/version.py')

    return config
```

#### 4b. Extensions
For extensions, again in the `def configuration` function in `setup.py`, locate the EXTENSIONS section, and add your extensions:

```python
# setup.py
...
def configuration(parent_package='', top_path=None):
    ...
    # EXTENSIONS
    # ========================
    # Fortran code that is NOT being compiled with f2py - it is being built as a fortran function that will be imported into C code
    config.add_library('fcwa_convert', sources='src/skimu/read/_extensions/cwa_convert.f95')
    # C code that contains the necessary CPython API calls to allow it to be imported and used in python
    config.add_extension(
        'skimu/read/_extensions/cwa_convert',  # note the path WITHOUT src/
        sources='src/skimu/read/_extensions/cwa_convert.c',  # note the path WITH src/
        libraries=['fcwa_convert']  # link the previously built fortran library
    )
    # standard C code extension that does not use a fortran library. 
    # Adding a Fortran extension follows the same syntax (numpy will do the heavy lifting for whatever compilation is required)
    config.add_extension(
        'skimu/read/_extensions/bin_convert',
        sources='src/skimu/read/_extensions/bin_convert.c'
    )

    # dealing with Cython extensions. 
    if os.environ.get('CYTHONIZE', 'False') == 'True':
        # if the environment variable was set, generate .c files from cython .pyx files. 
        # this is not necessary as the .c files are distributed with the code, but is available as an option in the off chance
        # that the .c files are not up to date
        from Cython.Build import cythonize  # only import if we need, as otherwise CYTHON isn't required as a requirement

        for pyxf in list(Path('.').rglob('*/features/lib/_cython/*.pyx')):
            cythonize(str(pyxf), compiler_directives={'language_level': 3})  # create a c file from the cython file

    # Either way, get a list of the cython .c files and add each as an extension to be compiled
    for cf in list(Path('.').rglob('*/features/lib/_cython/*.c')):
        config.add_extension(
            str(Path(*cf.parts[1:]).with_suffix('')),
            sources=[str(cf)]
        )

    # ========================
    ...

    return config
```

### 5. Documentation

Documentation is fairly simple - most of the documentation will already be done if you document your function calls/classes. The one place (noted above) to be careuful is when documenting classes - the docstring *needs* to go directly under the class definition to have a clean output for the autogenerated documentation.

Then, to document the whole module, a docstring is created in the `__init__.py` file:

```python
# src/skimu/preprocessing/__init__.py
"""
IMU PreProcessing (:mod:`skimu.preprocessing`)
====================================

.. currentmodule:: skimu.preprocessing

Pipeline gait processing
------------------------

.. autosummary::
    :toctree: generated/

    PreProcessing

Headline 2
----------
contents
"""
from skimu.preprocessing.preprocessing import PreProcessing
```

The docstring is written like a `.rst` header file (which is how it will get interpreted). For an example in `skimu`, see the [gait init file](src/skimu/gait/__init__.py).  For an example of a good module documentation from NumPY, see the [FFT](https://numpy.org/doc/stable/reference/routines.fft.html) page.

With this documentation written, the last thing is to add a short `.rst` file inside the actual documentation folder, which will instruct `sphinx` to read the documentation for this module.

```rst
.. _this_file: docs/ref/preprocessing.rst

.. automodule:: skimu.preprocessing
    :ignore-module-all:

```

Then add this new file to `docs/ref/index.rst`:

```rst
.. _this_file: docs/ref/index.rst
.. _skimu api reference

API Reference
================

.. toctree::
  :maxdepth: 2

  gait
  preprocessing

```

And thats it! If you want to generate the documentation, make sure you have all the [requirements](docs/requirements.txt) installed, and run:

```shell script
# make sure you're in the docs folder
cd docs
# generate the html documentation
make html
```

The generated HTML documentation is then in `docs/build/html`. Simply open `docs/build/html/index.html` in your browser and navigate around the documentation.

## Adding Tests for a new module
In order to make sure that any tests are run on installed versions of `scikit-imu`, the test directory is outside the `src` directory. Again, convenience base process testing classes are available to make setting up testing easy and quick.  All testing is done using Pytest

The first step is to create a new directory for your new module under the `test` directory, and add an `__init__.py` file to allow for relative importing, the test file, and a `conftest.py` file if desired:

```
scikit-imu
├── src
├── test
│   └──preprocessing
│       ├──  __init__.py
│       ├──  conftest.py
│       └── test_preprocessing.py
```

Inside `test_preprocessing.py`, import the base process testing class, set a few options, and the basic tests will be completed (when provided sample and truth data!):

```python
# test/preprocessing/test_preprocessing.py
import pytest

from ..base_conftest import *  # import BaseProcessTester, and resolve_data_path - a useful utility for making sure tests run in all 3 possible locations

from skimu.preprocessing import PreProcess


class TestPreProcess(BaseProcessTester):
    @classmethod
    def setup_class(cls):
        super().setup_class()  # make sure to call the super method

        # override specific necessary attributes
        """ resolve_data_path() takes 2 arguments
        1. the file name
        2. the module name (ie the folder name this script is in)
        """
        cls.sample_data_file = resolve_data_path('test_data.h5', 'preprocess')  
        cls.truth_data_file = resolve_data_path('test_data.h5', 'preprocess')  # can be the same file as sample data
        cls.truth_suffix = None  # if not none, means that the truth data is in the path "Truth/{truth_suffix}" in the truth h5 file
        cls.truth_data_keys = [  # list of keys to check against 
            'accel'
        ]
        cls.test_results = False  # we want to test the first return argument, not the "results"

        cls.process = PreProcess(attr1=5, attr2=10)
    
    """
    Adding additional tests, for errors, edge cases, anything that can't be accomplished with the 
    provided default structure can easily be accomplished by simply defining new functions under this class.
    Note that if the default .test method is not working for your use case, just overwrite it.
    """
    def test_error_missing_necessary_item(self):
        with pytest.raises(KeyError):
            PreProcess().predict({'not_necessary_item': 5})
```

This file would be all that is needed to test that the output `accel` values match those contained in the truth file.

### Sample/Truth data file format
The sample and truth data files are h5 files, with the below formats/keys:

```
sample.h5  # can have one or more of any of the below keys
├── time
├── accel
├── gyro
└──temperature
```

The `BaseProcessTester` class will automatically look for these keys. If you need to specify more/other keys, add the below line:
```python
@clsmethod
def setup_class(cls):
    ...
    cls.sample_data_keys.extend([
        'extra_key1',
        'extra_key2
    ])
    ...
```

The truth data is very similar. For our example (testing `accel` against its truth value):

```
truth.h5
├── Truth
│   └──accel
```

Alternatively, if `cls.truth_suffix` had been set to something else, ie `preproc`, then the structure would be as follows:

```
truth.h5
├── Truth
│   └──preproc
│       └──accel
```

the sample and truth files can be combined into 1 file:

```
sample_truth.h5
├── Truth
│   └──accel
├── time
├── accel
├── gyro
└──temperature
```

