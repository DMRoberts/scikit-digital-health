{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from pathlib import Path\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from PfyMU.gait.train_classifier.core import load_datasets\n",
    "from PfyMU.features import *\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_band_filter(x, fs):\n",
    "    sos = butter(\n",
    "        1, \n",
    "        [2 * 0.25 / fs, 2 * 5 / fs], \n",
    "        btype='band', \n",
    "        output='sos'\n",
    "    )\n",
    "    \n",
    "    return sosfiltfilt(sos, np.linalg.norm(x, axis=1))\n",
    "\n",
    "steps = {\n",
    "    'walking': 0.4,\n",
    "    'walking-impaired': 0.2,\n",
    "    'sitting': 900,\n",
    "    'standing': 300,\n",
    "    'stairs-ascending': 0.3,\n",
    "    'stairs-descending': 0.3,\n",
    "    'cycling-50W': 0.3,\n",
    "    'cycling-100W': 0.3,\n",
    "    'default': 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path('/home/lukasadamowicz/Documents/Datasets/processed')\n",
    "\n",
    "datasets = [\n",
    "    base_path / 'bluesky2',\n",
    "    base_path / 'daliac',\n",
    "    base_path / 'ltmm',\n",
    "    base_path / 'usc-had'\n",
    "]\n",
    "\n",
    "X, Y, subjects, activities = load_datasets(\n",
    "    paths=datasets,\n",
    "    goal_fs=50.0,\n",
    "    window_length=3.0,\n",
    "    window_step=steps,\n",
    "    acc_mag=False,\n",
    "    signal_function=mag_band_filter\n",
    ")\n",
    "\n",
    "Y2 = Y.copy()\n",
    "Y2[['stair' in i for i in activities]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomize validation/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(5)\n",
    "rnd_subjects = [i for i in np.unique(subjects) if np.unique(activities[subjects==i]).size > 3]\n",
    "random.shuffle(rnd_subjects)\n",
    "\n",
    "training_masks, validation_masks, testing_masks = [], [], []\n",
    "\n",
    "for i in range(0, len(rnd_subjects), 4):\n",
    "    trm = np.ones(len(subjects), dtype='bool')\n",
    "    vm = np.zeros_like(trm, dtype='bool')\n",
    "    tem = np.zeros_like(trm, dtype='bool')\n",
    "    \n",
    "    for j in range(4):\n",
    "        trm &= subjects != rnd_subjects[i + j]\n",
    "        if j < 2:\n",
    "            vm |= subjects == rnd_subjects[i + j]\n",
    "        else:\n",
    "            tem |= subjects == rnd_subjects[i + j]\n",
    "    \n",
    "    training_masks.append(trm)\n",
    "    validation_masks.append(vm)\n",
    "    testing_masks.append(tem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FB = Bank(window_length=None, window_step=None)\n",
    "\n",
    "FB.load('final_features.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feat, fnames = FB.compute(X, fs=50.0, windowed=True, columns=[''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {}\n",
    "with open('final_lgb_params.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.strip('\\n').split(':')\n",
    "\n",
    "        if '.' in parts[1]:\n",
    "            best_params[parts[0]] = float(parts[1])\n",
    "        elif parts[1].isnumeric():\n",
    "            best_params[parts[0]] = int(parts[1])\n",
    "        else:\n",
    "            best_params[parts[0]] = parts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set performance\n",
      "F1:   95.4      99.5  TP:   93.9      99.7  FP:    2.1       0.6\n",
      "F1:   92.5      97.8  TP:   95.9     100.0  FP:    4.2       2.8\n",
      "F1:   93.0      96.4  TP:   93.1      96.4  FP:    3.1       2.2\n",
      "F1:   91.9      97.5  TP:   97.6     100.0  FP:    5.2       3.2\n",
      "F1:   82.3      80.7  TP:   70.6      68.2  FP:    2.4       2.1\n",
      "F1:   94.3      97.3  TP:   89.5      95.3  FP:    0.6       0.9\n",
      "F1:   87.6      97.6  TP:   79.0      95.4  FP:    1.0       0.4\n",
      "F1:   94.3      99.0  TP:   95.4      98.2  FP:    7.3       0.4\n",
      "F1:   94.1      97.1  TP:   96.7      98.0  FP:    5.1       3.6\n",
      "F1:   93.3      98.7  TP:   94.2      98.3  FP:    3.5       2.1\n",
      "F1:   80.4      95.6  TP:   75.2     100.0  FP:    5.1       6.8\n",
      "\n",
      " --------------------------------------------------\n",
      "Mean (SD) F1: 90.8(4.9)   96.1(5.0)\n",
      "Mean (SD) TP: 89.2(9.1)   95.4(8.8)\n",
      "Mean (SD) FP: 3.6(1.9)   2.3(1.8)\n"
     ]
    }
   ],
   "source": [
    "f1, f1_2 = [], []\n",
    "tp, tp2 = [], []\n",
    "fp, fp2 = [], []\n",
    "\n",
    "print('Validation set performance')\n",
    "for trm, vm, tem in zip(training_masks, validation_masks, testing_masks):\n",
    "    lgb_cls = lgb.LGBMClassifier(**best_params)\n",
    "    lgb_cls2 = lgb.LGBMClassifier(**best_params)\n",
    "    \n",
    "    lgb_cls.fit(X_feat[trm], Y[trm])\n",
    "    lgb_cls2.fit(X_feat[trm], Y2[trm])\n",
    "    \n",
    "    y_pred = lgb_cls.predict_proba(X_feat[vm])[:, 1]\n",
    "    y2_pred = lgb_cls2.predict_proba(X_feat[vm])[:, 1]\n",
    "    \n",
    "    # compute metrics\n",
    "    f1.append(f1_score(Y[vm], y_pred > thresh))\n",
    "    f1_2.append(f1_score(Y2[vm], y2_pred > thresh))\n",
    "    tp.append((Y[vm] & (y_pred > thresh)).sum() / Y[vm].sum())\n",
    "    tp2.append((Y2[vm] & (y2_pred > thresh)).sum() / Y2[vm].sum())\n",
    "    fp.append((~Y[vm].astype(bool) & (y_pred > thresh)).sum() / (Y[vm].size - Y[vm].sum()))\n",
    "    fp2.append((~Y2[vm].astype(bool) & (y2_pred > thresh)).sum() / (Y2[vm].size - Y2[vm].sum()))\n",
    "    \n",
    "    print(f'F1: {f1[-1]*100:6.1f}{f1_2[-1]*100:10.1f}', end='')\n",
    "    print(f'  TP: {tp[-1]*100:6.1f}{tp2[-1]*100:10.1f}', end='')\n",
    "    print(f'  FP: {fp[-1]*100:6.1f}{fp2[-1]*100:10.1f}')\n",
    "    \n",
    "print('\\n', '-' * 50)\n",
    "print(f'Mean (SD) F1: {np.mean(f1)*100:.1f}({np.std(f1)*100:.1f})   {np.mean(f1_2)*100:.1f}({np.std(f1_2)*100:.1f})')\n",
    "print(f'Mean (SD) TP: {np.mean(tp)*100:.1f}({np.std(tp)*100:.1f})   {np.mean(tp2)*100:.1f}({np.std(tp2)*100:.1f})')\n",
    "print(f'Mean (SD) FP: {np.mean(fp)*100:.1f}({np.std(fp)*100:.1f})   {np.mean(fp2)*100:.1f}({np.std(fp2)*100:.1f})')\n",
    "\n",
    "df = pd.DataFrame(columns=['Model', 'Metric', 'Score'])\n",
    "df['Metric'] = ['F1'] * len(f1) + ['TP'] * len(tp) + ['FP'] * len(fp)\n",
    "df['Score'] = f1 + tp + fp\n",
    "df['Model'] = 'V2'\n",
    "\n",
    "df.to_csv('v2_validation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set performance\n",
      "F1:   91.0      95.3  TP:   86.8      91.5  FP:    4.0       0.6\n",
      "F1:   86.1      96.9  TP:   89.4      98.5  FP:    7.4       3.2\n",
      "F1:   91.1      95.5  TP:   85.1      97.1  FP:    1.4      10.0\n",
      "F1:   87.1      98.7  TP:   87.0      97.7  FP:    7.9       0.5\n",
      "F1:   84.9      95.9  TP:   96.2      99.6  FP:   13.4      12.7\n",
      "F1:   73.1      89.3  TP:   60.6      80.8  FP:    3.6       0.7\n",
      "F1:   90.4      99.2  TP:   84.6      99.6  FP:    1.7       1.5\n",
      "F1:   88.6      96.6  TP:   88.8      96.6  FP:    3.7       1.9\n",
      "F1:   59.8      60.0  TP:   44.4      44.7  FP:    5.2       5.4\n",
      "F1:   97.3      98.6  TP:   96.0      98.0  FP:    1.6       1.3\n",
      "F1:   92.6      99.0  TP:   91.9     100.0  FP:    3.2       1.6\n",
      "\n",
      " --------------------------------------------------\n",
      "Mean (SD) F1: 85.6(10.0)   93.2(10.8)\n",
      "Mean (SD) TP: 82.8(15.2)   91.3(15.7)\n",
      "Mean (SD) FP: 4.8(3.4)   3.6(3.9)\n"
     ]
    }
   ],
   "source": [
    "f1, f1_2 = [], []\n",
    "tp, tp2 = [], []\n",
    "fp, fp2 = [], []\n",
    "\n",
    "print('Test set performance')\n",
    "for trm, vm, tem in zip(training_masks, validation_masks, testing_masks):\n",
    "    lgb_cls = lgb.LGBMClassifier(n_estimators=125, **best_params)\n",
    "    lgb_cls2 = lgb.LGBMClassifier(n_estimators=125, **best_params)\n",
    "    \n",
    "    lgb_cls.fit(X_feat[trm], Y[trm])\n",
    "    lgb_cls2.fit(X_feat[trm], Y2[trm])\n",
    "    \n",
    "    y_pred = lgb_cls.predict_proba(X_feat[tem])[:, 1]\n",
    "    y2_pred = lgb_cls2.predict_proba(X_feat[tem])[:, 1]\n",
    "    \n",
    "    y_true = Y[tem].astype(bool)\n",
    "    y2_true = Y2[tem].astype(bool)\n",
    "    \n",
    "    # compute metrics\n",
    "    f1.append(f1_score(y_true, y_pred > thresh))\n",
    "    f1_2.append(f1_score(y2_true, y2_pred > thresh))\n",
    "    tp.append((y_true & (y_pred > thresh)).sum() / y_true.sum())\n",
    "    tp2.append((y2_true & (y2_pred > thresh)).sum() / y2_true.sum())\n",
    "    fp.append((~y_true & (y_pred > thresh)).sum() / (y_true.size - y_true.sum()))\n",
    "    fp2.append((~y2_true & (y2_pred > thresh)).sum() / (y2_true.size - y2_true.sum()))\n",
    "    \n",
    "    print(f'F1: {f1[-1]*100:6.1f}{f1_2[-1]*100:10.1f}', end='')\n",
    "    print(f'  TP: {tp[-1]*100:6.1f}{tp2[-1]*100:10.1f}', end='')\n",
    "    print(f'  FP: {fp[-1]*100:6.1f}{fp2[-1]*100:10.1f}')\n",
    "    \n",
    "print('\\n', '-' * 50)\n",
    "print(f'Mean (SD) F1: {np.mean(f1)*100:.1f}({np.std(f1)*100:.1f})   {np.mean(f1_2)*100:.1f}({np.std(f1_2)*100:.1f})')\n",
    "print(f'Mean (SD) TP: {np.mean(tp)*100:.1f}({np.std(tp)*100:.1f})   {np.mean(tp2)*100:.1f}({np.std(tp2)*100:.1f})')\n",
    "print(f'Mean (SD) FP: {np.mean(fp)*100:.1f}({np.std(fp)*100:.1f})   {np.mean(fp2)*100:.1f}({np.std(fp2)*100:.1f})')\n",
    "\n",
    "df = pd.DataFrame(columns=['Model', 'Metric', 'Score'])\n",
    "df['Metric'] = ['F1'] * len(f1) + ['TP'] * len(tp) + ['FP'] * len(fp)\n",
    "df['Score'] = f1 + tp + fp\n",
    "df['Model'] = 'V2'\n",
    "\n",
    "df.to_csv('v2_test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set performance\n",
      "F1:   95.3  TP:   93.7  FP:    2.1\n",
      "F1:   92.6  TP:   95.9  FP:    4.1\n",
      "F1:   93.2  TP:   93.4  FP:    3.1\n",
      "F1:   92.0  TP:   97.8  FP:    5.2\n",
      "F1:   82.0  TP:   70.1  FP:    2.4\n",
      "F1:   94.9  TP:   90.4  FP:    0.3\n",
      "F1:   87.8  TP:   79.3  FP:    0.9\n",
      "F1:   94.3  TP:   95.3  FP:    7.0\n",
      "F1:   94.4  TP:   97.0  FP:    4.8\n",
      "F1:   93.5  TP:   94.4  FP:    3.4\n",
      "F1:   79.5  TP:   74.3  FP:    5.5\n",
      "\n",
      " --------------------------------------------------\n",
      "Mean (SD) F1: 90.9(5.2)\n",
      "Mean (SD) TP: 89.2(9.4)\n",
      "Mean (SD) FP: 3.5(1.9)\n"
     ]
    }
   ],
   "source": [
    "f1, tp, fp = [], [], []\n",
    "params = best_params.copy()\n",
    "params['n_estimators'] = 125\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['subsample_for_bin'] = 200000\n",
    "params['subsample'] = 1.0\n",
    "params['subsample_freq'] = 0\n",
    "params['colsample_bytree'] = 1.0\n",
    "params['objective'] = 'binary'\n",
    "params['metrics'] = ['binary']\n",
    "\n",
    "print('Validation set performance')\n",
    "for trm, vm, tem in zip(training_masks, validation_masks, testing_masks):\n",
    "    x_dst = lgb.Dataset(X_feat[trm], label=Y[trm])\n",
    "    bst = lgb.train(params, x_dst, num_boost_round=100)\n",
    "    \n",
    "    y_pred = bst.predict(X_feat[vm])\n",
    "    \n",
    "    # compute metrics\n",
    "    f1.append(f1_score(Y[vm], y_pred > thresh))\n",
    "    tp.append((Y[vm] & (y_pred > thresh)).sum() / Y[vm].sum())\n",
    "    fp.append((~Y[vm].astype(bool) & (y_pred > thresh)).sum() / (Y[vm].size - Y[vm].sum()))\n",
    "    \n",
    "    print(f'F1: {f1[-1]*100:6.1f}', end='')\n",
    "    print(f'  TP: {tp[-1]*100:6.1f}', end='')\n",
    "    print(f'  FP: {fp[-1]*100:6.1f}')\n",
    "    \n",
    "print('\\n', '-' * 50)\n",
    "print(f'Mean (SD) F1: {np.mean(f1)*100:.1f}({np.std(f1)*100:.1f})')\n",
    "print(f'Mean (SD) TP: {np.mean(tp)*100:.1f}({np.std(tp)*100:.1f})')\n",
    "print(f'Mean (SD) FP: {np.mean(fp)*100:.1f}({np.std(fp)*100:.1f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f367ff93b20>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = best_params.copy()\n",
    "params['n_estimators'] = 125\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['subsample_for_bin'] = 200000\n",
    "params['subsample'] = 1.0\n",
    "params['subsample_freq'] = 0\n",
    "params['colsample_bytree'] = 1.0\n",
    "params['objective'] = 'binary'\n",
    "params['metrics'] = ['binary']\n",
    "\n",
    "\n",
    "x_dst = lgb.Dataset(X_feat, label=Y)\n",
    "bst = lgb.train(params, x_dst, num_boost_round=100)\n",
    "\n",
    "bst.save_model('lgbm_gait_classifier_no-stairs.lgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PfyMU",
   "language": "python",
   "name": "pfymu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
