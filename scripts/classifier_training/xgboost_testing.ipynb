{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ast\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_LOSO_runner(data, params, train_masks, validate_masks, test_masks, test=False):\n",
    "    feats = data.drop(['Subject', 'Activity', 'Label'], axis=1)\n",
    "    labels = data.Label\n",
    "    \n",
    "    # metrics\n",
    "    accu, prec, rec, f1 = [], [], [], []\n",
    "    \n",
    "    # models\n",
    "    mdls = []\n",
    "    \n",
    "    # predictions\n",
    "    predictions = []\n",
    "    truths = []\n",
    "    \n",
    "    # iterate over the masks\n",
    "    for mTrn, mVal, mTst in zip(train_masks, validate_masks, test_masks):\n",
    "        dtrain = xgb.DMatrix(feats.loc[mTrn], label=labels.loc[mTrn])\n",
    "        dval = xgb.DMatrix(feats.loc[mVal], label=labels.loc[mVal])\n",
    "        if test:\n",
    "            dtest = xgb.DMatrix(feats.loc[mTst], label=labels.loc[mTst])\n",
    "        \n",
    "        eval_list = [(dtrain, 'train')]\n",
    "        bst = xgb.train(params, dtrain, 80, eval_list, verbose_eval=False)\n",
    "        \n",
    "        if test:\n",
    "            y_pred_ = bst.predict(dtest)\n",
    "            y_pred =  y_pred_ > 0.5\n",
    "            y_true = labels.loc[mTst]\n",
    "        else:\n",
    "            y_pred_ = bst.predict(dval)\n",
    "            y_pred = y_pred_ > 0.5\n",
    "            y_true = labels.loc[mVal]\n",
    "        \n",
    "        accu.append(accuracy_score(y_true, y_pred, normalize=True))\n",
    "        prec.append(precision_score(y_true, y_pred))\n",
    "        rec.append(recall_score(y_true, y_pred))\n",
    "        f1.append(f1_score(y_true, y_pred))\n",
    "        \n",
    "        mdls.append(bst)\n",
    "        predictions.append(y_pred_)\n",
    "        truths.append(y_true)\n",
    "        \n",
    "    print(f'Average Accuracy: {np.mean(accu):.2f} ({np.std(accu):.2f})')\n",
    "    print(f'Average Precision: {np.mean(prec):.2f} ({np.std(prec):.2f})')\n",
    "    print(f'Average Recall: {np.mean(rec):.2f} ({np.std(rec):.2f})')\n",
    "    print(f'Average F1 Score: {np.mean(f1):.2f} ({np.std(f1):.2f})')\n",
    "        \n",
    "    return mdls, feats.columns, predictions, truths, accu, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf('../feature_exploration/features.h5', key='no_preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the subjects for which LOSO actually makes sense: those with multiple activities (ie more than just walking)\n",
    "gbc = data.groupby(['Subject', 'Activity'], as_index=False).count()\n",
    "loso_subjects = [i for i in gbc.Subject.unique() if gbc.loc[gbc.Subject == i].shape[0] > 3]\n",
    "\n",
    "random.seed(5)  # fix the generation so that its the same every time\n",
    "random.shuffle(loso_subjects)\n",
    "\n",
    "training_masks = []\n",
    "validation_masks = []\n",
    "testing_masks = []\n",
    "\n",
    "for i in range(0, len(loso_subjects), 3):\n",
    "    tr_m = np.ones(data.shape[0], dtype='bool')\n",
    "    v_m = np.zeros(data.shape[0], dtype='bool')\n",
    "    \n",
    "    for j in range(3):\n",
    "        tr_m &= (data.Subject != loso_subjects[i+j]).values\n",
    "    for j in range(2):\n",
    "        v_m |= (data.Subject == loso_subjects[i+j]).values\n",
    "    te_m = (data.Subject == loso_subjects[i+2]).values\n",
    "    \n",
    "    training_masks.append(tr_m)\n",
    "    validation_masks.append(v_m)\n",
    "    testing_masks.append(te_m)\n",
    "\n",
    "masks = (training_masks, validation_masks, testing_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the least important features (based on PPScore)\n",
    "data_subset = data.drop([\n",
    "    'Skewness',\n",
    "    'Kurtosis',\n",
    "    'LinearSlope',\n",
    "    'SpectralFlatness',\n",
    "    'Autocorrelation',\n",
    "    'RangeCountPercentage',\n",
    "    'ComplexityInvariantDistance',\n",
    "    'PowerSpectralSum',\n",
    "    'RatioBeyondRSigma',\n",
    "    'SignalEntropy',\n",
    "    'DominantFrequencyValue',\n",
    "    'JerkMetric',  # add mean cross rate, remove Jerkmetric (correlation with DimensionlessJerk)\n",
    "    'StdDev'  # add mean, remove StdDev (high correlation with RMS)\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvr = pd.read_csv('xgboost_cv_results_topfeats.csv', index_col=0)\n",
    "params = ast.literal_eval(\n",
    "    cvr.sort_values('rank_test_score').params.values[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.91 (0.05)\n",
      "Average Precision: 0.76 (0.12)\n",
      "Average Recall: 0.84 (0.16)\n",
      "Average F1 Score: 0.79 (0.11)\n"
     ]
    }
   ],
   "source": [
    "mdls, feats, preds, truths, *metrics = xgb_LOSO_runner(\n",
    "    data_subset,\n",
    "    params,\n",
    "    *(training_masks, validation_masks, testing_masks),\n",
    "    test=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59802434e7764c3f9f07933c18e1c22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4009526 0.4047655\n"
     ]
    }
   ],
   "source": [
    "f, (ax, ax1) = plt.subplots(ncols=2, figsize=(10, 6), sharey=True)\n",
    "\n",
    "opt_trsh = []\n",
    "\n",
    "i = 1\n",
    "for p_, t_ in zip(preds, truths):\n",
    "    fpr, tpr, thrsh = roc_curve(t_, p_)\n",
    "    \n",
    "    # compute optimal threshold based on Youdin's index\n",
    "    dist = tpr - fpr\n",
    "    \n",
    "    opt_trsh.append(thrsh[np.argmax(dist)])\n",
    "    \n",
    "    ax.plot(fpr, tpr, label=f'Fold{i:3d}: {auc(fpr, tpr):.2f}')\n",
    "    ax1.plot(thrsh, dist, label=f'Fold{i:3d}: {opt_trsh[-1]:.2f}')\n",
    "    i += 1\n",
    "\n",
    "ax.legend(loc='best')\n",
    "ax1.legend(loc=3)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax1.axvline(0.5, color='navy', lw=2, linestyle='--')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax1.set_xlim([0.0, 1.0])\n",
    "\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "f.tight_layout()\n",
    "\n",
    "print(np.mean(opt_trsh), np.median(opt_trsh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f555323351249ac9220f994ae4812af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 6), sharey=True)\n",
    "\n",
    "i = 1\n",
    "for p_, t_ in zip(preds, truths):\n",
    "    precision, recall, thrsh = precision_recall_curve(t_, p_)\n",
    "    ap = average_precision_score(t_, p_)\n",
    "    \n",
    "    ax.plot(recall, precision, label=f'Fold{i:3d}: {ap:.2f}')\n",
    "    i += 1\n",
    "    \n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = ax.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    ax.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "ax.legend(loc='best')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_acts = []\n",
    "fp_fold = []\n",
    "fp_certainty = []\n",
    "\n",
    "i = 1\n",
    "for mask, pred, truth in zip(validation_masks, preds, truths):\n",
    "    false_pos = mask.copy()\n",
    "    false_pos[false_pos] &= (pred > 0.5) & (truth == 0)\n",
    "    \n",
    "    fp_acts.extend(data.Activity.loc[false_pos].tolist())\n",
    "    fp_fold.extend([i] * false_pos.sum())\n",
    "    fp_certainty.extend(pred[false_pos[mask]])\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "fp = pd.DataFrame(data={\n",
    "    'Activity': fp_acts,\n",
    "    'Fold': fp_fold, \n",
    "    'Certainty': fp_certainty\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99385f3cfac4df080644ec317fcaab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "sns.boxplot(x='Activity', y='Certainty', hue='Fold', data=fp, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "ax.set_title('False Positive Certainty Distribution')\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PfyMU",
   "language": "python",
   "name": "pfymu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
