{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.svm import OneClassSVM, SVC\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, precision_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf('../feature_exploration/features.h5', key='no_preprocessing')\n",
    "feats = data.iloc[:, 3:]\n",
    "labels = data.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sitting', 'standing', 'walking', 'walking-impaired',\n",
       "       'standing-assisted', 'sit-to-stand', 'cycling-100W', 'cycling-50W',\n",
       "       'jumping-rope', 'lying', 'running-treadmill', 'stairs-ascending',\n",
       "       'stairs-descending', 'sweeping', 'vacuuming', 'washing-dishes',\n",
       "       'elevator-ascending', 'elevator-descending', 'jumping', 'running',\n",
       "       'sleeping', 'walking-left', 'walking-right'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Activity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the k-fold splits, but on subjects\n",
    "gbc = data.groupby(['Subject', 'Activity'], as_index=False).count()\n",
    "fold_subjects = [i for i in gbc.Subject.unique() if gbc.loc[gbc.Subject == i].shape[0] > 3]\n",
    "\n",
    "random.seed(5)  # fix the generation for repeatability\n",
    "random.shuffle(fold_subjects)\n",
    "\n",
    "trn_m, utrn_m, val_m, tst_m = [], [], [], []\n",
    "n_lo = 4\n",
    "for i in range(0, len(fold_subjects), n_lo):\n",
    "    trn_m.append(np.ones(data.shape[0], dtype='bool'))\n",
    "    utrn_m.append(np.ones(data.shape[0], dtype='bool'))\n",
    "    val_m.append(np.zeros(data.shape[0], dtype='bool'))\n",
    "    tst_m.append(np.zeros(data.shape[0], dtype='bool'))\n",
    "    \n",
    "    for j in range(n_lo):\n",
    "        trn_m[-1] &= (data.Subject != fold_subjects[i+j]).values\n",
    "        utrn_m[-1] &= (data.Subject != fold_subjects[i+j]).values\n",
    "    for j in range(n_lo - (n_lo // 2)):\n",
    "        val_m[-1] |= (data.Subject == fold_subjects[i+j]).values\n",
    "    for j in range(n_lo - (n_lo // 2), n_lo):\n",
    "        tst_m[-1] |= (data.Subject == fold_subjects[i+j]).values\n",
    "    \n",
    "    # make the unary (one-class) classifier mask to be \n",
    "    # only walking activities for the specific subjects\n",
    "    mask = np.zeros(data.shape[0], dtype='bool')\n",
    "    for a in [i for i in data.Activity.unique() if 'walking' in i]:\n",
    "        mask |= (data.Activity == a).values\n",
    "    utrn_m[-1] &= mask\n",
    "\n",
    "cv = tuple(zip(trn_m, val_m))\n",
    "ucv = tuple(zip(utrn_m, val_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uf1_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    y_pred[y_pred == -1] = 0\n",
    "    \n",
    "    return f1_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overground Walking Classification Summary\n",
    "\n",
    "This is a summary of the work-process for classifying overground gait from a lumbar accelerometer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Activity/Class balancing\n",
    "\n",
    "Window length: 3s\n",
    "Overlap: variable\n",
    "Sampling Frequency: 50Hz (downsampled where necessary)\n",
    "\n",
    "Loading data with a constand window for all activities didn't result in a nice distribution of activities, especially those which need to be captured for the classifier to work well, such as running, or stair ascent/descent.\n",
    "\n",
    "In order to provide a better balance of classes, the spacing or overlap between windows was adjust per activity to the following:\n",
    "\n",
    "| Activity             | Overlap |\n",
    "|----------------------|---------|\n",
    "| jumping-rope         | 0.15    |\n",
    "| stairs-descending    | 0.10    |\n",
    "| stairs-ascending     | 0.10    |\n",
    "| jumping              | 0.15    |\n",
    "| lying                | 0.15    |\n",
    "| elevator-ascending   | 0.15    |\n",
    "| elevator-descending  | 0.15    |\n",
    "| running              | 0.075   |\n",
    "| sweeping             | 0.15    |\n",
    "| standing             | 225     |\n",
    "| running-treadmill    | 0.10    |\n",
    "| cycling-50W          | 0.12    |\n",
    "| cycling-100W         | 0.12    |\n",
    "| walking-left         | 0.20    |\n",
    "| walking-right        | 0.20    |\n",
    "| walking-impaired     | 0.20    |\n",
    "| walking              | 0.25    |\n",
    "| sitting              | 400     |\n",
    "| default              | 0.50    |\n",
    "\n",
    "where a float indicates the % overlap between adjacent windows (ie 0.5 would be 50% overlap, or 75 samples between starts), and an integer indicates the number of samples between window starts (so 400 would exclude data of samples 151-400)\n",
    "\n",
    "This results in the following distribution of positive/negative samples (stair ascending/descending in the negative class for now):\n",
    "\n",
    "| Category    | Samples |\n",
    "|-------------|---------|\n",
    "| Total       | 105,029 |\n",
    "| Walking     | 32,781  |\n",
    "| Non-walking | 73,986  |\n",
    "|-------------|---------|\n",
    "| % walking   | 30.70%  |\n",
    "\n",
    "and the following specific activity breakdown:\n",
    "\n",
    "| Activity                 | Samples / Total   | % Samples |\n",
    "|--------------------------|-------------------|-----------|\n",
    "| sit-to-stand             |     30 / 105,029  |  0.00     |\n",
    "| standing-assisted        |    401 / 105,029  |  0.00     |\n",
    "| vacuuming                |    736 / 105,029  |  0.01     |\n",
    "| lying                    |    739 / 105,029  |  0.01     |\n",
    "| jumping-rope             |  1,373 / 105,029  |  0.01     |\n",
    "| washing-dishes           |  1,538 / 105,029  |  0.01     |\n",
    "| jumping                  |  1,869 / 105,029  |  0.02     |\n",
    "| sleeping                 |  2,245 / 105,029  |  0.02     |\n",
    "| elevator-descending      |  3,070 / 105,029  |  0.03     |\n",
    "| elevator-ascending       |  3,119 / 105,029  |  0.03     |\n",
    "| walking-left             |  3,774 / 105,029  |  0.04     |\n",
    "| standing                 |  3,875 / 105,029  |  0.04     |\n",
    "| walking-right            |  4,023 / 105,029  |  0.04     |\n",
    "| sweeping                 |  4,090 / 105,029  |  0.04     |\n",
    "| cycling-50W              |  6,251 / 105,029  |  0.06     |\n",
    "| cycling-100W             |  6,266 / 105,029  |  0.06     |\n",
    "| running                  |  6,764 / 105,029  |  0.06     |\n",
    "| sitting                  |  6,920 / 105,029  |  0.07     |\n",
    "| stairs-descending        |  7,336 / 105,029  |  0.07     |\n",
    "| running-treadmill        |  7,415 / 105,029  |  0.07     |\n",
    "| stairs-ascending         |  8,211 / 105,029  |  0.08     |\n",
    "| walking-impaired         |  8,241 / 105,029  |  0.08     |\n",
    "| walking                  | 16,743 / 105,029  |  0.16     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features were compute on the magnitude of the acceleration, computed per:\n",
    "$$a_{mag} = \\sqrt{a_x^2+a_y^2+a_z^2}$$\n",
    "\n",
    "The following features were computed:\n",
    "\n",
    "| Feature                      | Parameters | \n",
    "|------------------------------|------------|\n",
    "| Mean                         |                                         |\n",
    "| MeanCrossRate                |                                         |\n",
    "| StdDev                       |                                         |\n",
    "| Skewness                     |                                         |\n",
    "| Kurtosis                     |                                         |\n",
    "| Range                        |                                         |\n",
    "| IQR                          |                                         |\n",
    "| RMS                          |                                         |\n",
    "| Autocorrelation              | lag=1, normalize=True                   |\n",
    "| LinearSlope                  |                                         |\n",
    "| SignalEntropy                |                                         |\n",
    "| SampleEntropy                | m=4, r=1.0                              |\n",
    "| PermutationEntropy           | order=3, delay=1, normalize=True        |\n",
    "| ComplexityInvariantDistance  | normalize=True                          |\n",
    "| RangeCountPercentage         | range_min=0, range_max=1.0              |\n",
    "| RatioBeyondRSigma            | r=2.0                                   |\n",
    "| JerkMetric                   | normalize=True                          |\n",
    "| DimensionlessJerk            | log=True, signal_type='acceleration'    |\n",
    "| SPARC                        |                                         |\n",
    "| DominantFrequency            | low_cutoff=0.25, high_cutoff=5.0        |\n",
    "| DominantFrequencyValue       | low_cutoff=0.25, high_cutoff=5.0        |\n",
    "| PowerSpectralSum             | low_cutoff=0.25, high_cutoff=5.0        |\n",
    "| SpectralFlatness             | low_cutoff=0.25, high_cutoff=5.0        |\n",
    "| SpectralEntropy              | low_cutoff=0.25, high_cutoff=5.0        |\n",
    "| DetailPower                  | wavelet='coif4', freq_band=[1.0, 3.0]   |\n",
    "| DetailPowerRatio             | wavelet='coif4', freq_band=[1.0, 3.0]   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>TODO:</b> Explore effect of changing parameters. Could use PPScore to see effect on classification\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Selection\n",
    "\n",
    "Initial testing on binary classifiers SVM/RandomForest/XGBoost and one-class classifiers IsolationForest/OneClassSVM\n",
    "\n",
    "Initial testing was done with stair ascent/descent as the __*negative*__ class\n",
    "\n",
    "Overall performance showed that a binary classifier would achieve higher results. Initially dropping some features showed little/no change in performance, but as features were dropped based on feature performance in the classifiers this isn't surprising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1: 81.32\n"
     ]
    }
   ],
   "source": [
    "RF = RandomForestClassifier(n_estimators=20)\n",
    "cv_res = cross_val_score(\n",
    "    RF, \n",
    "    feats, \n",
    "    labels, \n",
    "    scoring=make_scorer(f1_score), \n",
    "    cv=cv, \n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f'Average F1: {np.mean(cv_res)*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b: SVM\n",
    "SVM takes a while to run\n",
    "\n",
    "Average F1: ~58.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1: 58.84\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(C=1.0, kernel='rbf')\n",
    "run = False\n",
    "if run:\n",
    "    cv_res = cross_val_score(\n",
    "        svm, \n",
    "        feats,\n",
    "        labels,\n",
    "        scoring=make_scorer(f1_score),\n",
    "        cv=cv,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    print(f'Average F1: {np.mean(cv_res)*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. XGBoost\n",
    "In the end showed similar performance to the random forest, but has the benefit of faster run times and better persistence/saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1: 84.01\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "cv_res = cross_val_score(\n",
    "    clf,\n",
    "    feats,\n",
    "    labels,\n",
    "    scoring=make_scorer(f1_score),\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f'Average F1: {np.mean(cv_res)*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d: Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1: 61.86\n"
     ]
    }
   ],
   "source": [
    "IF = IsolationForest(n_estimators=20)\n",
    "cv_res = cross_val_score(\n",
    "    IF,\n",
    "    feats,\n",
    "    labels,\n",
    "    scoring=uf1_scorer,\n",
    "    cv=ucv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f'Average F1: {np.mean(cv_res)*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e: One-class SVM\n",
    "Also takes a while\n",
    "\n",
    "Average F1: ~48.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1: 48.52\n"
     ]
    }
   ],
   "source": [
    "usvc = OneClassSVM(kernel='rbf')\n",
    "run = False\n",
    "if run:\n",
    "    cv_res = cross_val_score(\n",
    "        usvc,\n",
    "        feats,\n",
    "        labels,\n",
    "        scoring=uf1_scorer,\n",
    "        cv=ucv,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    print(f'Average F1: {np.mean(cv_res)*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the large number of classes, it isn't surprising that the binary classifiers end up working better, given that they should be able to account for the feature space occupied by the other activities better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PfyMU",
   "language": "python",
   "name": "pfymu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
