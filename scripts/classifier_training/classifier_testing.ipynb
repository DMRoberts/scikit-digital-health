{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random # shuffle the subjects\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.svm import OneClassSVM, SVC\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOSO_runner(data, model, train_masks, validate_masks, test_masks, test=False):    \n",
    "    # seperate the features out\n",
    "    feats = data.drop(['Subject', 'Activity', 'Label'], axis=1)\n",
    "    \n",
    "    # metrics\n",
    "    accu = []\n",
    "    prec = []\n",
    "    rec = []\n",
    "    f1 = []\n",
    "    \n",
    "    # return the models\n",
    "    mdls = []\n",
    "    \n",
    "    # iterate over groups of N_lo subjects to leave out\n",
    "    if test:\n",
    "        itr = zip(train_masks, test_masks)\n",
    "    else:\n",
    "        itr = zip(train_masks, validate_masks)\n",
    "    for train_mask, pred_mask in itr:\n",
    "        clm = clone(model)\n",
    "        clm.fit(feats.loc[train_mask], data.Label[train_mask])\n",
    "        \n",
    "        y_pred = clm.predict(feats.loc[pred_mask])\n",
    "        y_true = data.Label[pred_mask]\n",
    "        \n",
    "        accu.append(accuracy_score(y_true, y_pred, normalize=True))\n",
    "        prec.append(precision_score(y_true, y_pred))\n",
    "        rec.append(recall_score(y_true, y_pred))\n",
    "        f1.append(f1_score(y_true, y_pred))\n",
    "        \n",
    "        mdls.append(clm)\n",
    "    \n",
    "    print(f'Average Accuracy: {np.mean(accu):.2f} ({np.std(accu):.2f})')\n",
    "    print(f'Average Precision: {np.mean(prec):.2f} ({np.std(prec):.2f})')\n",
    "    print(f'Average Recall: {np.mean(rec):.2f} ({np.std(rec):.2f})')\n",
    "    print(f'Average F1 Score: {np.mean(f1):.2f} ({np.std(f1):.2f})')\n",
    "        \n",
    "    return mdls, feats.columns, accu, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_LOSO_runner(data, train_masks, validate_masks, test_masks, test=False):\n",
    "    feats = data.drop(['Subject', 'Activity', 'Label'], axis=1)\n",
    "    labels = data.Label\n",
    "    \n",
    "    # metrics\n",
    "    accu = []\n",
    "    prec = []\n",
    "    rec = []\n",
    "    f1 = []\n",
    "    \n",
    "    # return the models\n",
    "    mdls = []\n",
    "    \n",
    "    # iterate over groups of N_lo subjects to leave out    \n",
    "    for train_mask, val_mask, test_mask in zip(train_masks, validate_masks, test_masks):\n",
    "        dtrain = xgb.DMatrix(feats.loc[train_mask], label=labels.loc[train_mask])\n",
    "        dval = xgb.DMatrix(feats.loc[val_mask], label=labels.loc[val_mask])\n",
    "        if test:\n",
    "            dtest = xgb.DMatrix(feats.loc[test_mask], label=labels.loc[test_mask])\n",
    "\n",
    "        params = {\n",
    "            'max_depth': 8,  # default\n",
    "            'eval_metric': ['rmse', 'auc']\n",
    "        }\n",
    "        eval_list = [(dval, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "        bst = xgb.train(params, dtrain, 10, eval_list, verbose_eval=False)\n",
    "        \n",
    "        if test:\n",
    "            y_pred = bst.predict(dtest) > 0.5\n",
    "            y_true = labels.loc[test_mask]\n",
    "        else:\n",
    "            y_pred = bst.predict(dval) > 0.5\n",
    "            y_true = labels.loc[val_mask]\n",
    "        \n",
    "        accu.append(accuracy_score(y_true, y_pred, normalize=True))\n",
    "        prec.append(precision_score(y_true, y_pred))\n",
    "        rec.append(recall_score(y_true, y_pred))\n",
    "        f1.append(f1_score(y_true, y_pred))\n",
    "        \n",
    "        mdls.append(bst)\n",
    "        \n",
    "    print(f'Average Accuracy: {np.mean(accu):.2f} ({np.std(accu):.2f})')\n",
    "    print(f'Average Precision: {np.mean(prec):.2f} ({np.std(prec):.2f})')\n",
    "    print(f'Average Recall: {np.mean(rec):.2f} ({np.std(rec):.2f})')\n",
    "    print(f'Average F1 Score: {np.mean(f1):.2f} ({np.std(f1):.2f})')\n",
    "        \n",
    "    return mdls, feats.columns, accu, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unary_LOSO_runner(data, model, train_masks, validate_masks, test_masks, test=False):\n",
    "    feats = data.drop(['Subject', 'Activity', 'Label'], axis=1)\n",
    "    labels = data.Label\n",
    "    \n",
    "    # metrics\n",
    "    accu = []\n",
    "    prec = []\n",
    "    rec = []\n",
    "    f1 = []\n",
    "    \n",
    "    # return the models\n",
    "    mdls = []\n",
    "    \n",
    "    # iterate over groups of N_lo subjects to leave out    \n",
    "    if test:\n",
    "        itr = zip(train_masks, test_masks)\n",
    "    else:\n",
    "        itr = zip(train_masks, validate_masks)\n",
    "    for train_mask, pred_mask in itr:\n",
    "        clm = clone(model)\n",
    "        clm.fit(feats.loc[train_mask & (labels == 1)])\n",
    "        \n",
    "        y_pred = clm.predict(feats.loc[pred_mask])\n",
    "        y_pred[y_pred == -1] = 0\n",
    "        y_true = data.Label[pred_mask]\n",
    "        \n",
    "        accu.append(accuracy_score(y_true, y_pred, normalize=True))\n",
    "        prec.append(precision_score(y_true, y_pred))\n",
    "        rec.append(recall_score(y_true, y_pred))\n",
    "        f1.append(f1_score(y_true, y_pred))\n",
    "        \n",
    "        mdls.append(clm)\n",
    "        \n",
    "    print(f'Average Accuracy: {np.mean(accu):.2f} ({np.std(accu):.2f})')\n",
    "    print(f'Average Precision: {np.mean(prec):.2f} ({np.std(prec):.2f})')\n",
    "    print(f'Average Recall: {np.mean(rec):.2f} ({np.std(rec):.2f})')\n",
    "    print(f'Average F1 Score: {np.mean(f1):.2f} ({np.std(f1):.2f})')\n",
    "        \n",
    "    return mdls, feats.columns, accu, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_RF_feature_importances(models, features):\n",
    "    ft_impt = pd.DataFrame(columns=['Feature', 'Importance'])\n",
    "    ft_impt['Importance'] = [i for L in models for i in L.feature_importances_]\n",
    "    ft_impt['Feature'] = np.tile(features, len(models))\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.violinplot(x='Feature', y='Importance', data=ft_impt, ax=ax, width=1, scale='width', bw=0.25)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xgb_feature_gains(models):\n",
    "    df = pd.DataFrame(columns=['Feature', 'Split', 'Gain'])\n",
    "    \n",
    "    df['Feature'] = [i for d in models for i in d.get_score(importance_type='gain')]\n",
    "    df['Split'] = np.repeat(np.arange(len(models)), len(models[0].get_score()))\n",
    "    df['Gain'] = [i for d in models for i in d.get_score(importance_type='gain').values()]\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.boxplot(x='Feature', y='Gain', data=df)\n",
    "    \n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf('../feature_exploration/features.h5', key='no_preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the subjects for which LOSO actually makes sense: those with multiple activities (ie more than just walking)\n",
    "gbc = data.groupby(['Subject', 'Activity'], as_index=False).count()\n",
    "loso_subjects = [i for i in gbc.Subject.unique() if gbc.loc[gbc.Subject == i].shape[0] > 3]\n",
    "\n",
    "random.seed(5)  # fix the generation so that its the same every time\n",
    "random.shuffle(loso_subjects)\n",
    "\n",
    "training_masks = []\n",
    "validation_masks = []\n",
    "testing_masks = []\n",
    "\n",
    "for i in range(0, len(loso_subjects), 3):\n",
    "    tr_m = np.ones(data.shape[0], dtype='bool')\n",
    "    v_m = np.zeros(data.shape[0], dtype='bool')\n",
    "    \n",
    "    for j in range(3):\n",
    "        tr_m &= data.Subject != loso_subjects[i+j]\n",
    "    for j in range(2):\n",
    "        v_m |= data.Subject == loso_subjects[i+j]\n",
    "    te_m = data.Subject == loso_subjects[i+2]\n",
    "    \n",
    "    training_masks.append(tr_m)\n",
    "    validation_masks.append(v_m)\n",
    "    testing_masks.append(te_m)\n",
    "\n",
    "masks = (training_masks, validation_masks, testing_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain expert feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_data = data.drop([\n",
    "    'Mean',\n",
    "    'Skewness',\n",
    "    'Kurtosis',\n",
    "    'RMS',\n",
    "    'LinearSlope',\n",
    "    'ComplexityInvariantDistance',\n",
    "    'RangeCountPercentage',\n",
    "    'RatioBeyondRSigma',\n",
    "    'JerkMetric',\n",
    "    'DominantFrequencyValue',\n",
    "    'DetailPower',\n",
    "    'SignalEntropy',\n",
    "    'SpectralFlatness'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_data = data.copy()\n",
    "rs_data.iloc[:, 3:] = RobustScaler().fit_transform(data.iloc[:, 3:])\n",
    "ss_data = data.copy()\n",
    "ss_data.iloc[:, 3:] = StandardScaler().fit_transform(data.iloc[:, 3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "pca_data = data.iloc[:, :3].copy()\n",
    "pca_data = pca_data.merge(pd.DataFrame(pca.fit_transform(rs_data.iloc[:, 3:])), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.92 (0.03)\n",
      "Average Precision: 0.84 (0.09)\n",
      "Average Recall: 0.81 (0.15)\n",
      "Average F1 Score: 0.81 (0.10)\n",
      "\n",
      "Robust Scaling\n",
      "Average Accuracy: 0.92 (0.03)\n",
      "Average Precision: 0.85 (0.09)\n",
      "Average Recall: 0.81 (0.14)\n",
      "Average F1 Score: 0.82 (0.09)\n",
      "\n",
      "Standard Scaling\n",
      "Average Accuracy: 0.92 (0.03)\n",
      "Average Precision: 0.85 (0.09)\n",
      "Average Recall: 0.81 (0.15)\n",
      "Average F1 Score: 0.82 (0.09)\n",
      "\n",
      "PCA (robust scaling)\n",
      "Average Accuracy: 0.89 (0.04)\n",
      "Average Precision: 0.76 (0.11)\n",
      "Average Recall: 0.77 (0.14)\n",
      "Average F1 Score: 0.76 (0.10)\n"
     ]
    }
   ],
   "source": [
    "RF = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "rf_mdls, fts, *rf_metrics = LOSO_runner(data, RF, training_masks, validation_masks, testing_masks, test=False)\n",
    "print('\\nRobust Scaling')\n",
    "rf_rs_mdls, rs_fts, *rf_rs_metrics = LOSO_runner(rs_data, RF, *masks, test=False)\n",
    "print('\\nStandard Scaling')\n",
    "rf_ss_mdls, ss_fts, *rf_ss_metrics = LOSO_runner(ss_data, RF, *masks, test=False)\n",
    "print('\\nPCA (robust scaling)')\n",
    "rf_pca_mdls, pca_fts, *rf_pca_metrics = LOSO_runner(pca_data, RF, *masks, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a29155784f4c93ae0320bce94c61a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04da5ff435ce4cd7b745e9e5ba5cdc34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31a5888bbc84e6eabb72ff9baf72a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_RF_feature_importances(rf_mdls, fts)\n",
    "plot_RF_feature_importances(rf_rs_mdls, fts)\n",
    "plot_RF_feature_importances(rf_ss_mdls, fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_subset = data.drop(['Skewness', 'Kurtosis', 'Autocorrelation', 'LinearSlope', 'SignalEntropy', \n",
    "#                          'ComplexityInvariantDistance', 'RangeCountPercentage', 'RatioBeyondRSigma',\n",
    "#                          'SpectralFlatness', 'Mean', 'MeanCrossRate', 'DominantFrequencyValue', 'RMS', 'DetailPowerRatio'], axis=1)\n",
    "data_subset = data.drop([\n",
    "    'LinearSlope',\n",
    "    'RatioBeyondRSigma',\n",
    "    'SpectralFlatness',\n",
    "    'ComplexityInvariantDistance',\n",
    "    'Kurtosis',\n",
    "    'Autocorrelation',\n",
    "    'DominantFrequencyValue',\n",
    "    'SignalEntropy',\n",
    "    'RangeCountPercentage'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.92 (0.03)\n",
      "Average Precision: 0.84 (0.09)\n",
      "Average Recall: 0.81 (0.14)\n",
      "Average F1 Score: 0.81 (0.09)\n"
     ]
    }
   ],
   "source": [
    "# check performance after dropping some of the less important features\n",
    "rfsub_mdls, sub_fts, *rfsub_metrics = LOSO_runner(data_subset, RF, training_masks, validation_masks, testing_masks, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd751bd910f741f29ff7ae82faf66bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_RF_feature_importances(rfsub_mdls, sub_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.91 (0.04)\n",
      "Average Precision: 0.80 (0.10)\n",
      "Average Recall: 0.80 (0.14)\n",
      "Average F1 Score: 0.79 (0.09)\n"
     ]
    }
   ],
   "source": [
    "# check performance using DE features\n",
    "rfde_mdls, de_fts, *rfde_metrics = LOSO_runner(de_data, RF, training_masks, validation_masks, testing_masks, test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.87 (0.05)\n",
      "Average Precision: 0.82 (0.05)\n",
      "Average Recall: 0.88 (0.15)\n",
      "Average F1 Score: 0.84 (0.09)\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(C=1.0, kernel='rbf')\n",
    "\n",
    "svc_mdls, fts, *svc_metrics = LOSO_runner(data_subset, svc, training_masks, validation_masks, testing_masks, test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Scaling\n",
      "Average Accuracy: 0.91 (0.04)\n",
      "Average Precision: 0.81 (0.10)\n",
      "Average Recall: 0.82 (0.15)\n",
      "Average F1 Score: 0.80 (0.09)\n",
      "\n",
      "Robust Scaling\n",
      "Average Accuracy: 0.91 (0.04)\n",
      "Average Precision: 0.81 (0.10)\n",
      "Average Recall: 0.82 (0.15)\n",
      "Average F1 Score: 0.80 (0.09)\n",
      "\n",
      "Standard Scaling\n",
      "Average Accuracy: 0.91 (0.04)\n",
      "Average Precision: 0.81 (0.10)\n",
      "Average Recall: 0.82 (0.15)\n",
      "Average F1 Score: 0.80 (0.09)\n",
      "\n",
      "PCA (robust scaling)\n",
      "Average Accuracy: 0.88 (0.04)\n",
      "Average Precision: 0.71 (0.13)\n",
      "Average Recall: 0.81 (0.14)\n",
      "Average F1 Score: 0.75 (0.10)\n"
     ]
    }
   ],
   "source": [
    "print('No Scaling')\n",
    "xgb_mdls, xgb_fts, xgb_accu, xgb_prec, xgb_rec, xgb_f1 = xgb_LOSO_runner(data, training_masks, validation_masks, testing_masks, test=False)\n",
    "print('\\nRobust Scaling')\n",
    "xgb_rs_mdls, xgb_rs_fts, *xgb_rs_metr = xgb_LOSO_runner(rs_data, *masks, test=False)\n",
    "print('\\nStandard Scaling')\n",
    "xgb_ss_mdls, xgb_ss_fts, *xgb_ss_metr = xgb_LOSO_runner(ss_data, *masks, test=False)\n",
    "print('\\nPCA (robust scaling)')\n",
    "xgb_pca_mdls, xgb_pca_fts, *xgb_pca_metr = xgb_LOSO_runner(pca_data, *masks, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_subset_2 = data.drop(['Mean', 'RatioBeyondRSigma', 'SpectralEntropy', 'SpectralFlatness', 'LinearSlope', 'RMS', 'Skewness', \n",
    "#                            'Kurtosis', 'Autocorrelation', 'DominantFrequencyValue', 'IQR', \n",
    "#                            'MeanCrossRate', 'ComplexityInvariantDistance', 'RangeCountPercentage', 'SignalEntropy', \n",
    "#                            'DimensionlessJerk', 'DetailPowerRatio', 'JerkMetric', 'SampleEntropy'], axis=1)\n",
    "data_subset_2 = data.drop([\n",
    "    'LinearSlope',\n",
    "    'SPARC',\n",
    "    'RatioBeyondRSigma',\n",
    "    'Mean',\n",
    "    'Kurtosis', \n",
    "    'PowerSpectralSum',\n",
    "    'IQR',\n",
    "    'RangeCountPercentage',\n",
    "    'DominantFrequencyValue',\n",
    "    'ComplexityInvariantDistance'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.91 (0.04)\n",
      "Average Precision: 0.79 (0.11)\n",
      "Average Recall: 0.84 (0.14)\n",
      "Average F1 Score: 0.80 (0.09)\n"
     ]
    }
   ],
   "source": [
    "xgb_s_mdls, xgb_s_fts, xgb_s_accu, xgb_s_prec, xgb_s_rec, xgb_s_f1 = xgb_LOSO_runner(data_subset_2, training_masks, validation_masks, testing_masks, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfe1500b00e4a289d5a3999f55bd1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_xgb_feature_gains(xgb_s_mdls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.90 (0.04)\n",
      "Average Precision: 0.76 (0.12)\n",
      "Average Recall: 0.83 (0.15)\n",
      "Average F1 Score: 0.78 (0.10)\n"
     ]
    }
   ],
   "source": [
    "xgb_de_mdls, xgb_de_fts, xgb_de_accu, xgb_de_prec, xgb_de_rec, xgb_de_f1 = xgb_LOSO_runner(de_data, training_masks, validation_masks, testing_masks, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-class Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.71 (0.11)\n",
      "Average Precision: 0.44 (0.20)\n",
      "Average Recall: 0.84 (0.12)\n",
      "Average F1 Score: 0.56 (0.18)\n"
     ]
    }
   ],
   "source": [
    "IF = IsolationForest(n_estimators=20)\n",
    "\n",
    "if_mdls, if_fts, *if_metr = unary_LOSO_runner(data, IF, *masks, test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.80 (0.05)\n",
      "Average Precision: 0.54 (0.16)\n",
      "Average Recall: 0.44 (0.19)\n",
      "Average F1 Score: 0.46 (0.16)\n"
     ]
    }
   ],
   "source": [
    "usvc = OneClassSVM(kernel='rbf')\n",
    "\n",
    "usvc_mdls, _, *usvc_metr = unary_LOSO_runner(data, usvc, *masks, test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Random Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, loguniform, randint\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = xgb.XGBClassifier()\n",
    "param_distributions = {\n",
    "    'max_depth': [3, 5, 7, 9, 11],\n",
    "    'learning_rate': loguniform(1e-6, 1e-1),\n",
    "    'booster': ['gbtree', 'gblinear', 'dart'],\n",
    "}\n",
    "n_iter = 15 \n",
    "scoring = make_scorer(f1_score)\n",
    "n_jobs = -1  # use all possible cores\n",
    "refit = False  # don't want to refit on the whole dataset afterwards\n",
    "verbose = 2\n",
    "cv = ((training_masks[i], validation_masks[i]) for i in range(len(training_masks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomizedSearchCV(\n",
    "    estimator,\n",
    "    param_distributions,\n",
    "    cv=cv,\n",
    "    n_iter=n_iter,\n",
    "    scoring=scoring,\n",
    "    n_jobs=n_jobs,\n",
    "    refit=refit,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 15 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed: 17.6min finished\n"
     ]
    }
   ],
   "source": [
    "search = clf.fit(data.iloc[:, 3:], data.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(data=search.cv_results_)\n",
    "cv_results.to_csv('xgboost_cv_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier Random CV search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier()\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(5, 100),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [4, 7, 10, 13, 16, None],\n",
    "    'min_samples_split': [2, 10, 20, 50, 100, 500, 1000],\n",
    "    'min_samples_leaf': [1, 2, 4, 8, 16, 32]\n",
    "}\n",
    "n_iter = 25\n",
    "scoring = make_scorer(f1_score)\n",
    "n_jobs = -1  # use all possible cores\n",
    "refit = False  # don't want to refit on the whole dataset afterwards\n",
    "verbose = 2\n",
    "cv = ((training_masks[i], validation_masks[i]) for i in range(len(training_masks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomizedSearchCV(\n",
    "    estimator,\n",
    "    param_distributions,\n",
    "    cv=cv,\n",
    "    n_iter=n_iter,\n",
    "    scoring=scoring,\n",
    "    n_jobs=n_jobs,\n",
    "    refit=refit,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 25 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   30.7s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-51041d89e2f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pfymu/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pfymu/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pfymu/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1528\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m         evaluate_candidates(ParameterSampler(\n\u001b[0m\u001b[1;32m   1530\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m             random_state=self.random_state))\n",
      "\u001b[0;32m~/miniconda3/envs/pfymu/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pfymu/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pfymu/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pfymu/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pfymu/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pfymu/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "search = clf.fit(data.iloc[:, 3:], data.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PfyMU",
   "language": "python",
   "name": "pfymu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
