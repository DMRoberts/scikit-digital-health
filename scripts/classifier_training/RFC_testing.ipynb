{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ast\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOSO_runner(data, model, train_masks, validate_masks, test_masks, test=False):\n",
    "    # seperate features out\n",
    "    feats = data.drop(['Subject', 'Activity', 'Label'], axis=1)\n",
    "    \n",
    "    # metrics\n",
    "    accu, prec, rec, f1 = [], [], [], []\n",
    "    \n",
    "    # models\n",
    "    mdls = []\n",
    "    \n",
    "    # predictions\n",
    "    predictions, truths = [], []\n",
    "    \n",
    "    # iterate over masks\n",
    "    if test:\n",
    "        itr = zip(train_masks, test_masks)\n",
    "    else:\n",
    "        itr = zip(train_masks, validate_masks)\n",
    "    \n",
    "    for mtrain, mpred in itr:\n",
    "        clm = clone(model)\n",
    "        clm.fit(feats.loc[mtrain], data.Label[mtrain])\n",
    "        \n",
    "        y_pred_ = clm.predict_proba(feats.loc[mpred])\n",
    "        y_pred = clm.predict(feats.loc[mpred])\n",
    "        y_true = data.Label[mpred]\n",
    "        \n",
    "        predictions.append(y_pred_)\n",
    "        truths.append(y_true)\n",
    "        \n",
    "        accu.append(accuracy_score(y_true, y_pred, normalize=True))\n",
    "        prec.append(precision_score(y_true, y_pred))\n",
    "        rec.append(recall_score(y_true, y_pred))\n",
    "        f1.append(f1_score(y_true, y_pred))\n",
    "        \n",
    "        mdls.append(clm)\n",
    "        \n",
    "    print(f'Average Accuracy: {np.mean(accu):.2f} ({np.std(accu):.2f})')\n",
    "    print(f'Average Precision: {np.mean(prec):.2f} ({np.std(prec):.2f})')\n",
    "    print(f'Average Recall: {np.mean(rec):.2f} ({np.std(rec):.2f})')\n",
    "    print(f'Average F1 Score: {np.mean(f1):.2f} ({np.std(f1):.2f})')\n",
    "        \n",
    "    return mdls, feats.columns, predictions, truths, accu, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf('../feature_exploration/features.h5', key='no_preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the subjects for which LOSO actually makes sense: those with multiple activities (ie more than just walking)\n",
    "gbc = data.groupby(['Subject', 'Activity'], as_index=False).count()\n",
    "loso_subjects = [i for i in gbc.Subject.unique() if gbc.loc[gbc.Subject == i].shape[0] > 3]\n",
    "\n",
    "random.seed(5)  # fix the generation so that its the same every time\n",
    "random.shuffle(loso_subjects)\n",
    "\n",
    "training_masks = []\n",
    "validation_masks = []\n",
    "testing_masks = []\n",
    "\n",
    "for i in range(0, len(loso_subjects), 3):\n",
    "    tr_m = np.ones(data.shape[0], dtype='bool')\n",
    "    v_m = np.zeros(data.shape[0], dtype='bool')\n",
    "    \n",
    "    for j in range(3):\n",
    "        tr_m &= data.Subject != loso_subjects[i+j]\n",
    "    for j in range(2):\n",
    "        v_m |= data.Subject == loso_subjects[i+j]\n",
    "    te_m = data.Subject == loso_subjects[i+2]\n",
    "    \n",
    "    training_masks.append(tr_m)\n",
    "    validation_masks.append(v_m)\n",
    "    testing_masks.append(te_m)\n",
    "\n",
    "masks = (training_masks, validation_masks, testing_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the least important features (based on PPScore)\n",
    "data_subset = data.drop([\n",
    "    'Skewness',\n",
    "    'Kurtosis',\n",
    "    'LinearSlope',\n",
    "    'SpectralFlatness',\n",
    "    'Autocorrelation',\n",
    "    'RangeCountPercentage',\n",
    "    'ComplexityInvariantDistance',\n",
    "    'PowerSpectralSum',\n",
    "    'RatioBeyondRSigma',\n",
    "    'SignalEntropy',\n",
    "    'DominantFrequencyValue',\n",
    "    'JerkMetric',  # add mean cross rate, remove Jerkmetric (correlation with DimensionlessJerk)\n",
    "    'StdDev'  # add mean, remove StdDev (high correlation with RMS)\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvr = pd.read_csv('rfc_cv_results_topfeats.csv', index_col=0)\n",
    "params = ast.literal_eval(\n",
    "    cvr.sort_values('rank_test_score').params.values[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.91 (0.05)\n",
      "Average Precision: 0.79 (0.12)\n",
      "Average Recall: 0.83 (0.14)\n",
      "Average F1 Score: 0.80 (0.10)\n"
     ]
    }
   ],
   "source": [
    "_, _, preds, truths, *_ = LOSO_runner(\n",
    "    data_subset,\n",
    "    model,\n",
    "    *(training_masks, validation_masks, testing_masks),\n",
    "    test=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2606fece91749f18e264fcacc3615e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38483951364473534 0.3871361425424525\n"
     ]
    }
   ],
   "source": [
    "f, (ax, ax1) = plt.subplots(ncols=2, figsize=(10, 6), sharey=True)\n",
    "\n",
    "opt_trsh = []\n",
    "\n",
    "i = 1\n",
    "for p_, t_ in zip(preds, truths):\n",
    "    fpr, tpr, thrsh = roc_curve(t_, p_[:, 1])\n",
    "    \n",
    "    # compute optimal threshold based on Youdin's index\n",
    "    dist = tpr - fpr\n",
    "    \n",
    "    opt_trsh.append(thrsh[np.argmax(dist)])\n",
    "    \n",
    "    ax.plot(fpr, tpr, label=f'Fold{i:3d}: {auc(fpr, tpr):.2f}')\n",
    "    ax1.plot(thrsh, dist, label=f'Fold{i:3d}: {opt_trsh[-1]:.2f}')\n",
    "    i += 1\n",
    "\n",
    "ax.legend(loc='best')\n",
    "ax1.legend(loc=3)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax1.axvline(0.5, color='navy', lw=2, linestyle='--')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax1.set_xlim([0.0, 1.0])\n",
    "\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "f.tight_layout()\n",
    "\n",
    "print(np.mean(opt_trsh), np.median(opt_trsh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e2117a96fa450b8ccc16172380af04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 6), sharey=True)\n",
    "\n",
    "i = 1\n",
    "for p_, t_ in zip(preds, truths):\n",
    "    precision, recall, thrsh = precision_recall_curve(t_, p_[:, 1])\n",
    "    ap = average_precision_score(t_, p_[:, 1])\n",
    "    \n",
    "    ax.plot(recall, precision, label=f'Fold{i:3d}: {ap:.2f}')\n",
    "    i += 1\n",
    "    \n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = ax.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    ax.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "ax.legend(loc='best')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PfyMU",
   "language": "python",
   "name": "pfymu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
