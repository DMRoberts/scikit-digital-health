{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from PfyMU.gait.train_classifier.core import load_datasets\n",
    "from PfyMU.features import *\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = {\n",
    "    'jumping-rope': 0.15,\n",
    "    'stairs-descending': 0.1,\n",
    "    'stairs-ascending': 0.1,\n",
    "    'jumping': 0.15,\n",
    "    'lying': 0.15,\n",
    "    'elevator-ascending': 0.15,\n",
    "    'elevator-descending': 0.15,\n",
    "    'running': 0.075,\n",
    "    'sweeping': 0.15,\n",
    "    'standing': 225,\n",
    "    'running-treadmill': 0.1,\n",
    "    'cycling-50W': 0.12,\n",
    "    'cycling-100W': 0.12,\n",
    "    'walking-left': 0.2,\n",
    "    'walking-right': 0.2,\n",
    "    'walking-impaired': 0.2,\n",
    "    'walking': 0.25,\n",
    "    'sitting': 400,\n",
    "    'default': 0.5\n",
    "}\n",
    "\n",
    "steps = {\n",
    "    'walking': 0.4,\n",
    "    'walking-impaired': 0.2,\n",
    "    'sitting': 900,\n",
    "    'standing': 300,\n",
    "    'stairs-ascending': 0.3,\n",
    "    'stairs-descending': 0.3,\n",
    "    'cycling-50W': 0.3,\n",
    "    'cycling-100W': 0.3,\n",
    "    'default': 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gait_sets_path = Path('/Users/adamol/Documents/Datasets/gait/processed')\n",
    "gait_sets_path = Path('/home/lukasadamowicz/Documents/Datasets/processed')\n",
    "\n",
    "datasets = [\n",
    "    gait_sets_path / 'bluesky2',\n",
    "    gait_sets_path / 'daliac',\n",
    "    gait_sets_path / 'ltmm',\n",
    "    gait_sets_path / 'usc-had'\n",
    "]\n",
    "\n",
    "X, Y, subjects, activities = load_datasets(\n",
    "    datasets, \n",
    "    goal_fs=50.0, \n",
    "    acc_mag=True, \n",
    "    window_length=3.0, \n",
    "    window_step=steps\n",
    ")\n",
    "\n",
    "# make stair-climbing in the positive class\n",
    "mask = (activities == 'stairs-ascending') | (activities == 'stairs-descending')\n",
    "Y_inc_str = Y.copy()\n",
    "Y_inc_str[mask] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples (3.0s windows):  43163\n",
      "Total walking samples:  20518\n",
      "Total non-walking samples:  22645 \n",
      "\n",
      "% walking samples: 47.54\n"
     ]
    }
   ],
   "source": [
    "print('Total samples (3.0s windows): ', Y.size)\n",
    "print('Total walking samples: ', Y.sum())\n",
    "print('Total non-walking samples: ', Y.size - Y.sum(), '\\n')\n",
    "print(f'% walking samples: {Y.sum() / Y.size * 100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sit-to-stand             :    16 / 43163    0.00\n",
      "standing-assisted        :   207 / 43163    0.00\n",
      "jumping-rope             :   212 / 43163    0.00\n",
      "jumping                  :   311 / 43163    0.01\n",
      "vacuuming                :   376 / 43163    0.01\n",
      "lying                    :   378 / 43163    0.01\n",
      "elevator-descending      :   475 / 43163    0.01\n",
      "elevator-ascending       :   491 / 43163    0.01\n",
      "running                  :   541 / 43163    0.01\n",
      "sweeping                 :   612 / 43163    0.01\n",
      "running-treadmill        :   755 / 43163    0.02\n",
      "washing-dishes           :   776 / 43163    0.02\n",
      "walking-left             :   787 / 43163    0.02\n",
      "walking-right            :   842 / 43163    0.02\n",
      "sleeping                 :  1126 / 43163    0.03\n",
      "stairs-descending        :  2477 / 43163    0.06\n",
      "cycling-50W              :  2509 / 43163    0.06\n",
      "cycling-100W             :  2515 / 43163    0.06\n",
      "stairs-ascending         :  2763 / 43163    0.06\n",
      "standing                 :  2953 / 43163    0.07\n",
      "sitting                  :  3152 / 43163    0.07\n",
      "walking-impaired         :  8241 / 43163    0.19\n",
      "walking                  : 10648 / 43163    0.25\n"
     ]
    }
   ],
   "source": [
    "unq_act, act_ct = np.unique(activities, return_counts=True)\n",
    "N = np.sum(act_ct)\n",
    "si = np.argsort(act_ct)\n",
    "for a, c in zip(unq_act[si], act_ct[si]):\n",
    "    print(f'{a:25s}: {c:5d} / {N:5d}{c/N:8.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_df = pd.DataFrame(data={'Subject': subjects, 'Activity': activities})\n",
    "sa_df['col1'] = 1.0\n",
    "\n",
    "# get the subjects for which LOSO actually makes sense: those with multiple activities (ie more than just walking)\n",
    "gbc = sa_df.groupby(['Subject', 'Activity'], as_index=False).count()\n",
    "loso_subjects = [i for i in gbc.Subject.unique() if gbc.loc[gbc.Subject == i].shape[0] > 3]\n",
    "\n",
    "random.seed(184751029)  # fix the generation so that its the same every time\n",
    "random.shuffle(loso_subjects)\n",
    "\n",
    "training_masks = []\n",
    "validation_masks = []\n",
    "testing_masks = []\n",
    "\n",
    "# for sub in np.unique(subjects):\n",
    "#     training_masks.append(np.array(subjects) != sub)\n",
    "#     validation_masks.append(np.array(subjects) == sub)\n",
    "\n",
    "for i in range(0, len(loso_subjects), 4):\n",
    "    tr_m = np.ones(sa_df.shape[0], dtype='bool')\n",
    "    v_m = np.zeros(sa_df.shape[0], dtype='bool')\n",
    "    te_m = np.zeros(sa_df.shape[0], dtype='bool')\n",
    "    \n",
    "    for j in range(4):\n",
    "        tr_m &= (sa_df.Subject != loso_subjects[i+j]).values\n",
    "    for j in range(2):\n",
    "        v_m |= (sa_df.Subject == loso_subjects[i+j]).values\n",
    "    for j in range(2):\n",
    "        te_m |= (sa_df.Subject == loso_subjects[i+j+2]).values\n",
    "    \n",
    "    training_masks.append(tr_m)\n",
    "    validation_masks.append(v_m)\n",
    "    testing_masks.append(te_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FB = Bank(window_length=None, window_step=None)\n",
    "\n",
    "# add features\n",
    "FB + Mean()\n",
    "FB + MeanCrossRate()\n",
    "FB + StdDev()\n",
    "FB + Skewness()\n",
    "FB + Kurtosis()\n",
    "FB + Range()\n",
    "FB + IQR()\n",
    "FB + RMS()\n",
    "FB + LinearSlope()\n",
    "FB + SignalEntropy()\n",
    "FB + SPARC()\n",
    "FB + ComplexityInvariantDistance(normalize=True)\n",
    "FB + JerkMetric(normalize=True)\n",
    "FB + DimensionlessJerk(log=True, signal_type='acceleration')\n",
    "\n",
    "FB + Autocorrelation(lag=1, normalize=True)\n",
    "FB + Autocorrelation(lag=15, normalize=True)\n",
    "FB + Autocorrelation(lag=14, normalize=True)\n",
    "FB + Autocorrelation(lag=12, normalize=True)\n",
    "\n",
    "FB + SampleEntropy(m=4, r=1.0)\n",
    "FB + SampleEntropy(m=2, r=0.75)\n",
    "FB + SampleEntropy(m=3, r=0.75)\n",
    "FB + SampleEntropy(m=2, r=0.5)\n",
    "FB + SampleEntropy(m=2, r=0.25)\n",
    "\n",
    "FB + PermutationEntropy(order=3, delay=1, normalize=True)\n",
    "FB + PermutationEntropy(order=5, delay=1, normalize=True)\n",
    "FB + PermutationEntropy(order=8, delay=1, normalize=True)\n",
    "FB + PermutationEntropy(order=10, delay=1, normalize=True)\n",
    "FB + PermutationEntropy(order=8, delay=2, normalize=True)\n",
    "FB + PermutationEntropy(order=8, delay=8, normalize=True)\n",
    "\n",
    "FB + RangeCountPercentage(range_min=0, range_max=1.0)\n",
    "FB + RangeCountPercentage(range_min=0.5, range_max=1.4)\n",
    "FB + RangeCountPercentage(range_min=0.3, range_max=1.4)\n",
    "FB + RangeCountPercentage(range_min=1, range_max=1.4)\n",
    "FB + RangeCountPercentage(range_min=0, range_max=1.5)\n",
    "\n",
    "FB + RatioBeyondRSigma(r=1.0)\n",
    "FB + RatioBeyondRSigma(r=2.5)\n",
    "FB + RatioBeyondRSigma(r=0.5)\n",
    "\n",
    "FB + DominantFrequency(low_cutoff=0.25, high_cutoff=5.0)\n",
    "FB + DominantFrequency(low_cutoff=1.0, high_cutoff=3.5)\n",
    "FB + DominantFrequency(low_cutoff=1.0, high_cutoff=3.0)\n",
    "FB + DominantFrequency(low_cutoff=1.5, high_cutoff=6.0)\n",
    "FB + DominantFrequency(low_cutoff=0.5, high_cutoff=3.0)\n",
    "\n",
    "FB + DominantFrequencyValue(low_cutoff=0.25, high_cutoff=5.0)\n",
    "FB + DominantFrequencyValue(low_cutoff=1.0, high_cutoff=3.5)\n",
    "FB + DominantFrequencyValue(low_cutoff=1.0, high_cutoff=3.0)\n",
    "FB + DominantFrequencyValue(low_cutoff=1.5, high_cutoff=6.0)\n",
    "FB + DominantFrequencyValue(low_cutoff=0.5, high_cutoff=3.0)\n",
    "\n",
    "FB + PowerSpectralSum(low_cutoff=0.25, high_cutoff=5.0)\n",
    "FB + PowerSpectralSum(low_cutoff=1.0, high_cutoff=3.0)\n",
    "FB + PowerSpectralSum(low_cutoff=1.5, high_cutoff=3.5)\n",
    "FB + PowerSpectralSum(low_cutoff=0.25, high_cutoff=4.0)\n",
    "FB + PowerSpectralSum(low_cutoff=0.25, high_cutoff=3.0)\n",
    "\n",
    "FB + SpectralFlatness(low_cutoff=0.25, high_cutoff=5.0)\n",
    "FB + SpectralFlatness(low_cutoff=0.0, high_cutoff=6.0)\n",
    "FB + SpectralFlatness(low_cutoff=0.0, high_cutoff=8.0)\n",
    "FB + SpectralFlatness(low_cutoff=0.0, high_cutoff=3.5)\n",
    "FB + SpectralFlatness(low_cutoff=0.5, high_cutoff=3.5)\n",
    "\n",
    "FB + SpectralEntropy(low_cutoff=0.25, high_cutoff=5.0)\n",
    "FB + SpectralEntropy(low_cutoff=0.0, high_cutoff=5.0)\n",
    "FB + SpectralEntropy(low_cutoff=0.0, high_cutoff=3.5)\n",
    "FB + SpectralEntropy(low_cutoff=0.25, high_cutoff=3.0)\n",
    "FB + SpectralEntropy(low_cutoff=1.5, high_cutoff=4.0)\n",
    "\n",
    "FB + DetailPower(wavelet='coif4', freq_band=[1.0, 3.0])\n",
    "\n",
    "FB + DetailPowerRatio(wavelet='coif4', freq_band=[1.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukasadamowicz/miniconda3/envs/pfymu/lib/python3.8/site-packages/pywt/_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_feat, feature_names = FB.compute(X, fs=50.0, windowed=True, columns=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = pd.DataFrame(\n",
    "    data=X_feat,\n",
    "    columns=feature_names,\n",
    "    dtype='float'\n",
    ")\n",
    "labels = Y\n",
    "labels_istrs = Y_inc_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "import lightgbm as lgb\n",
    "from boruta import BorutaPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 / 100\n",
      "Iteration: 2 / 100\n",
      "Iteration: 3 / 100\n",
      "Iteration: 4 / 100\n",
      "Iteration: 5 / 100\n",
      "Iteration: 6 / 100\n",
      "Iteration: 7 / 100\n",
      "Iteration: 8 / 100\n",
      "Iteration: 9 / 100\n",
      "Iteration: 10 / 100\n",
      "Iteration: 11 / 100\n",
      "Iteration: 12 / 100\n",
      "Iteration: 13 / 100\n",
      "Iteration: 14 / 100\n",
      "Iteration: 15 / 100\n",
      "Iteration: 16 / 100\n",
      "Iteration: 17 / 100\n",
      "Iteration: 18 / 100\n",
      "Iteration: 19 / 100\n",
      "Iteration: 20 / 100\n",
      "Iteration: 21 / 100\n",
      "Iteration: 22 / 100\n",
      "Iteration: 23 / 100\n",
      "Iteration: 24 / 100\n",
      "Iteration: 25 / 100\n",
      "Iteration: 26 / 100\n",
      "Iteration: 27 / 100\n",
      "Iteration: 28 / 100\n",
      "Iteration: 29 / 100\n",
      "Iteration: 30 / 100\n",
      "Iteration: 31 / 100\n",
      "Iteration: 32 / 100\n",
      "Iteration: 33 / 100\n",
      "Iteration: 34 / 100\n",
      "Iteration: 35 / 100\n",
      "Iteration: 36 / 100\n",
      "Iteration: 37 / 100\n",
      "Iteration: 38 / 100\n",
      "Iteration: 39 / 100\n",
      "Iteration: 40 / 100\n",
      "Iteration: 41 / 100\n",
      "Iteration: 42 / 100\n",
      "Iteration: 43 / 100\n",
      "Iteration: 44 / 100\n",
      "Iteration: 45 / 100\n",
      "Iteration: 46 / 100\n",
      "Iteration: 47 / 100\n",
      "Iteration: 48 / 100\n",
      "Iteration: 49 / 100\n",
      "Iteration: 50 / 100\n",
      "Iteration: 51 / 100\n",
      "Iteration: 52 / 100\n",
      "Iteration: 53 / 100\n",
      "Iteration: 54 / 100\n",
      "Iteration: 55 / 100\n",
      "Iteration: 56 / 100\n",
      "Iteration: 57 / 100\n",
      "Iteration: 58 / 100\n",
      "Iteration: 59 / 100\n",
      "Iteration: 60 / 100\n",
      "Iteration: 61 / 100\n",
      "Iteration: 62 / 100\n",
      "Iteration: 63 / 100\n",
      "Iteration: 64 / 100\n",
      "Iteration: 65 / 100\n",
      "Iteration: 66 / 100\n",
      "Iteration: 67 / 100\n",
      "Iteration: 68 / 100\n",
      "Iteration: 69 / 100\n",
      "Iteration: 70 / 100\n",
      "Iteration: 71 / 100\n",
      "Iteration: 72 / 100\n",
      "Iteration: 73 / 100\n",
      "Iteration: 74 / 100\n",
      "Iteration: 75 / 100\n",
      "Iteration: 76 / 100\n",
      "Iteration: 77 / 100\n",
      "Iteration: 78 / 100\n",
      "Iteration: 79 / 100\n",
      "Iteration: 80 / 100\n",
      "Iteration: 81 / 100\n",
      "Iteration: 82 / 100\n",
      "Iteration: 83 / 100\n",
      "Iteration: 84 / 100\n",
      "Iteration: 85 / 100\n",
      "Iteration: 86 / 100\n",
      "Iteration: 87 / 100\n",
      "Iteration: 88 / 100\n",
      "Iteration: 89 / 100\n",
      "Iteration: 90 / 100\n",
      "Iteration: 91 / 100\n",
      "Iteration: 92 / 100\n",
      "Iteration: 93 / 100\n",
      "Iteration: 94 / 100\n",
      "Iteration: 95 / 100\n",
      "Iteration: 96 / 100\n",
      "Iteration: 97 / 100\n",
      "Iteration: 98 / 100\n",
      "Iteration: 99 / 100\n",
      "\n",
      "\n",
      "BorutaPy finished running.\n",
      "\n",
      "Iteration: \t100 / 100\n",
      "Confirmed: \t62\n",
      "Tentative: \t2\n",
      "Rejected: \t0\n"
     ]
    }
   ],
   "source": [
    "rf = RFC(max_depth=5, random_state=30951)\n",
    "\n",
    "rfbor = BorutaPy(\n",
    "    estimator=rf,\n",
    "    n_estimators=20, \n",
    "    max_iter=100,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rfbor.fit(X_feat[training_masks[0]], Y[training_masks[0]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features in the green area:\n",
      "0 mean\n",
      "1 meancrossrate\n",
      "2 stddev\n",
      "3 skewness\n",
      "4 kurtosis\n",
      "5 range\n",
      "6 iqr\n",
      "7 rms\n",
      "8 linearslope\n",
      "9 signalentropy\n",
      "10 sparc_4_10.00_0.05\n",
      "11 complexityinvariantdistance_True\n",
      "12 jerkmetric\n",
      "13 dimensionlessjerk_True_acceleration\n",
      "14 autocorrelation_1_True\n",
      "15 autocorrelation_15_True\n",
      "16 autocorrelation_14_True\n",
      "17 autocorrelation_12_True\n",
      "18 sampleentropy_4_1.00\n",
      "19 sampleentropy_2_0.75\n",
      "20 sampleentropy_3_0.75\n",
      "21 sampleentropy_2_0.50\n",
      "22 sampleentropy_2_0.25\n",
      "23 permutationentropy_3_1_True\n",
      "24 permutationentropy_5_1_True\n",
      "25 permutationentropy_8_1_True\n",
      "26 permutationentropy_10_1_True\n",
      "27 permutationentropy_8_2_True\n",
      "28 permutationentropy_8_8_True\n",
      "29 rangecountpercentage_0_1.00\n",
      "30 rangecountpercentage_0.50_1.40\n",
      "31 rangecountpercentage_0.30_1.40\n",
      "32 rangecountpercentage_1_1.40\n",
      "33 rangecountpercentage_0_1.50\n",
      "34 ratiobeyondrsigma_1.00\n",
      "35 ratiobeyondrsigma_2.50\n",
      "36 ratiobeyondrsigma_0.50\n",
      "37 dominantfrequency_0.25_5.00\n",
      "38 dominantfrequency_1.00_3.50\n",
      "39 dominantfrequency_1.00_3.00\n",
      "40 dominantfrequency_1.50_6.00\n",
      "41 dominantfrequency_0.50_3.00\n",
      "42 dominantfrequencyvalue_0.25_5.00\n",
      "43 dominantfrequencyvalue_1.00_3.50\n",
      "44 dominantfrequencyvalue_1.00_3.00\n",
      "45 dominantfrequencyvalue_1.50_6.00\n",
      "46 dominantfrequencyvalue_0.50_3.00\n",
      "47 powerspectralsum_0.25_5.00\n",
      "48 powerspectralsum_1.00_3.00\n",
      "49 powerspectralsum_1.50_3.50\n",
      "50 powerspectralsum_0.25_4.00\n",
      "51 powerspectralsum_0.25_3.00\n",
      "52 spectralflatness_0.25_5.00\n",
      "53 spectralflatness_0.00_6.00\n",
      "54 spectralflatness_0.00_8.00\n",
      "55 spectralflatness_0.00_3.50\n",
      "56 spectralflatness_0.50_3.50\n",
      "57 spectralentropy_0.25_5.00\n",
      "58 spectralentropy_0.00_5.00\n",
      "59 spectralentropy_0.00_3.50\n",
      "60 spectralentropy_0.25_3.00\n",
      "61 spectralentropy_1.50_4.00\n",
      "62 detailpower_coif4_[1.0, 3.0]\n",
      "63 detailpowerratio_coif4_[1.0, 3.0]\n",
      "\n",
      "features in the blue area: ['permutationentropy_8_8_True']\n"
     ]
    }
   ],
   "source": [
    "green_area = np.array(feature_names)[rfbor.support_]\n",
    "blue_area = np.array(feature_names)[rfbor.support_weak_]\n",
    "\n",
    "print('features in the green area:')\n",
    "for i, g in enumerate(green_area):\n",
    "    print(i, g)\n",
    "    \n",
    "print('\\nfeatures in the blue area:', blue_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfbor.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PfyMU",
   "language": "python",
   "name": "pfymu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
